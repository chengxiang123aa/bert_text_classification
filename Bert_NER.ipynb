{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# NER是一种序列标注问题"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bfceab085250a4fc"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-18T10:00:57.006335500Z",
     "start_time": "2024-04-18T10:00:46.336670800Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Miniconda3\\envs\\conda_test\\lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.18) or chardet (5.2.0)/charset_normalizer (None) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n",
      "D:\\Miniconda3\\envs\\conda_test\\lib\\site-packages\\urllib3\\connectionpool.py:1061: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#NER数据命名实体识别数据\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from transformers import BertForTokenClassification\n",
    "from transformers import BertTokenizerFast\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "# 读取数据\n",
    "import os\n",
    "os.environ['CURL_CA_BUNDLE'] = ''\n",
    "df = pd.read_csv('ner.csv')\n",
    "#根据空格拆分标签，并将它们转换为列表\n",
    "labels = [i.split() for i in df['labels'].values.tolist()]\n",
    "# 检查数据集中有多少标签\n",
    "unique_labels = set()\n",
    "for lb in labels:\n",
    "  [unique_labels.add(i) for i in lb if i not in unique_labels]\n",
    "labels_to_ids = {k: v for v, k in enumerate(sorted(unique_labels))}\n",
    "#在能够使用 BERT 模型对 token 级别的实体进行分类之前，需要先进行数据预处理，包括两部分：tokenization 和调整标签以匹配 tokenization\n",
    "text = df['text'].values.tolist()\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def align_label(texts, labels,label_all_tokens=True):\n",
    "    # 首先tokenizer输入文本\n",
    "    tokenized_inputs = tokenizer(texts, padding='max_length', max_length=512, truncation=True)\n",
    "  # 获取word_ids\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "    previous_word_idx = None\n",
    "    label_ids = []\n",
    "    for word_idx in word_ids:\n",
    "        # 如果token不在word_ids内，则用 “-100” 填充\n",
    "        if word_idx is None:\n",
    "            label_ids.append(-100)\n",
    "        # 如果token在word_ids内，且word_idx不为None，则从labels_to_ids获取label id\n",
    "        elif word_idx != previous_word_idx:\n",
    "            try:\n",
    "                label_ids.append(labels_to_ids[labels[word_idx]])\n",
    "            except:\n",
    "                label_ids.append(-100)\n",
    "        # 如果token在word_ids内，且word_idx为None\n",
    "        else:\n",
    "            try:\n",
    "                label_ids.append(labels_to_ids[labels[word_idx]] if label_all_tokens else -100)\n",
    "            except:\n",
    "                label_ids.append(-100)\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    return label_ids\n",
    "\n",
    "\n",
    "\n",
    "# 构建自己的数据集类\n",
    "class DataSequence(Dataset):\n",
    "    def __init__(self, df):\n",
    "        # 根据空格拆分labels\n",
    "        lb = [i.split() for i in df['labels'].values.tolist()]\n",
    "        # tokenizer 向量化文本\n",
    "        txt = df['text'].values.tolist()\n",
    "        self.texts = [tokenizer(str(i),\n",
    "                               padding='max_length', max_length = 512,\n",
    "                                truncation=True, return_tensors=\"pt\") for i in txt]\n",
    "        # 对齐标签\n",
    "        self.labels = [align_label(i,j) for i,j in zip(txt, lb)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_data(self, idx):\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        return torch.LongTensor(self.labels[idx])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_data = self.get_batch_data(idx)\n",
    "        batch_labels = self.get_batch_labels(idx)\n",
    "        return batch_data, batch_labels\n",
    "\n",
    "df = df[0:1000]\n",
    "df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=42),\n",
    "                            [int(.8 * len(df)), int(.9 * len(df))])    #0.8训练集，0.1验证集,0.1测试集"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T10:00:57.042741500Z",
     "start_time": "2024-04-18T10:00:57.013479500Z"
    }
   },
   "id": "31c2a6ba77b3636d"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "## 建模\n",
    "class BertModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertModel, self).__init__()\n",
    "        self.bert = BertForTokenClassification.from_pretrained(\n",
    "                       'bert-base-cased',num_labels=len(unique_labels))\n",
    "\n",
    "    def forward(self, input_id, mask, label):\n",
    "        output = self.bert(input_ids=input_id, attention_mask=mask,\n",
    "                           labels=label, return_dict=False)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T10:00:57.054710100Z",
     "start_time": "2024-04-18T10:00:57.038682600Z"
    }
   },
   "id": "608ebf7f7538319b"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# 定义训练和验证集数据\n",
    "train_dataset = DataSequence(df_train)\n",
    "val_dataset = DataSequence(df_val)\n",
    "# 批量获取训练和验证集数据\n",
    "train_dataloader = DataLoader(train_dataset, num_workers=4, batch_size=1, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, num_workers=4, batch_size=1)\n",
    "# 判断是否使用GPU，如果有，尽量使用，可以加快训练速度\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T10:00:57.781074900Z",
     "start_time": "2024-04-18T10:00:57.052715100Z"
    }
   },
   "id": "4a25417851dedca4"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "def train_loop(model, df_train, df_val):\n",
    "  # 定义训练和验证集数据\n",
    "  train_dataset = DataSequence(df_train)\n",
    "  val_dataset = DataSequence(df_val)\n",
    "  # 批量获取训练和验证集数据\n",
    "  train_dataloader = DataLoader(train_dataset, num_workers=4, batch_size=1, shuffle=True)\n",
    "  val_dataloader = DataLoader(val_dataset, num_workers=4, batch_size=1)\n",
    "  # 判断是否使用GPU，如果有，尽量使用，可以加快训练速度\n",
    "  use_cuda = torch.cuda.is_available()\n",
    "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "  # 定义优化器\n",
    "  optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "  if use_cuda:\n",
    "    model = model.cuda()\n",
    "  # 开始训练循环\n",
    "  best_acc = 0\n",
    "  best_loss = 1000\n",
    "  for epoch_num in range(EPOCHS):\n",
    "\n",
    "    total_acc_train = 0\n",
    "    total_loss_train = 0\n",
    "    # 训练模型\n",
    "    model.train()\n",
    "    # 按批量循环训练模型\n",
    "    for train_data, train_label in tqdm(train_dataloader):\n",
    "      # 从train_data中获取mask和input_id\n",
    "      train_label = train_label[0].to(device)\n",
    "      mask = train_data['attention_mask'][0].to(device)\n",
    "      input_id = train_data['input_ids'][0].to(device)\n",
    "      # 梯度清零！！\n",
    "      optimizer.zero_grad()\n",
    "      # 输入模型训练结果：损失及分类概率\n",
    "      loss, logits = model(input_id, mask, train_label)\n",
    "      # 过滤掉特殊token及padding的token\n",
    "      logits_clean = logits[0][train_label != -100]\n",
    "      label_clean = train_label[train_label != -100]\n",
    "      # 获取最大概率值\n",
    "      predictions = logits_clean.argmax(dim=1)\n",
    "      # 计算准确率\n",
    "      acc = (predictions == label_clean).float().mean()\n",
    "      total_acc_train += acc\n",
    "      total_loss_train += loss.item()\n",
    "      # 反向传递\n",
    "      loss.backward()\n",
    "      # 参数更新\n",
    "      optimizer.step()\n",
    "    # 模型评估\n",
    "    model.eval()\n",
    "\n",
    "    total_acc_val = 0\n",
    "    total_loss_val = 0\n",
    "    for val_data, val_label in val_dataloader:\n",
    "      # 批量获取验证数据\n",
    "      val_label = val_label[0].to(device)\n",
    "      mask = val_data['attention_mask'][0].to(device)\n",
    "      input_id = val_data['input_ids'][0].to(device)\n",
    "      # 输出模型预测结果\n",
    "      loss, logits = model(input_id, mask, val_label)\n",
    "      # 清楚无效token对应的结果\n",
    "      logits_clean = logits[0][val_label != -100]\n",
    "      label_clean = val_label[val_label != -100]\n",
    "      # 获取概率值最大的预测\n",
    "      predictions = logits_clean.argmax(dim=1)\n",
    "      # 计算精度\n",
    "      acc = (predictions == label_clean).float().mean()\n",
    "      total_acc_val += acc\n",
    "      total_loss_val += loss.item()\n",
    "\n",
    "    val_accuracy = total_acc_val / len(df_val)\n",
    "    val_loss = total_loss_val / len(df_val)\n",
    "\n",
    "    print(\n",
    "      f'''Epochs: {epoch_num + 1} |\n",
    "                Loss: {total_loss_train / len(df_train): .3f} |\n",
    "                Accuracy: {total_acc_train / len(df_train): .3f} |\n",
    "                Val_Loss: {total_loss_val / len(df_val): .3f} |\n",
    "                Accuracy: {total_acc_val / len(df_val): .3f}''')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T10:00:57.821390Z",
     "start_time": "2024-04-18T10:00:57.796214500Z"
    }
   },
   "id": "b99619bfeb2bdefd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Miniconda3\\envs\\conda_test\\lib\\site-packages\\urllib3\\connectionpool.py:1061: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "D:\\Miniconda3\\envs\\conda_test\\lib\\site-packages\\urllib3\\connectionpool.py:1061: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 0/800 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "LEARNING_RATE = 1e-2\n",
    "EPOCHS = 5\n",
    "model = BertModel()\n",
    "train_loop(model, df_train, df_val)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-04-18T10:00:57.815360500Z"
    }
   },
   "id": "4df18a0d4bd66ba3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "872ff342d3daaa06"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
